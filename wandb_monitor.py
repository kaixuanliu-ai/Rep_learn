#!/usr/bin/env python3
"""
WandBå®æ—¶ç›‘æ§è„šæœ¬
"""
import os
import time
import argparse
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
import wandb

class WandBMonitor:
    def __init__(self, entity, projects, refresh_interval=60):
        self.entity = entity
        self.projects = projects if isinstance(projects, list) else [projects]
        self.refresh_interval = refresh_interval
        self.api = wandb.Api()
        
    def get_runs(self, project, filters=None):
        """è·å–é¡¹ç›®çš„è¿è¡Œ"""
        try:
            runs = self.api.runs(f"{self.entity}/{project}", filters=filters)
            return list(runs)
        except Exception as e:
            print(f"âŒ è·å–è¿è¡Œå¤±è´¥ {project}: {e}")
            return []
    
    def get_latest_run(self, project):
        """è·å–æœ€æ–°çš„è¿è¡Œ"""
        runs = self.get_runs(project)
        if runs:
            return runs[0]  # wandb.Api().runs() é»˜è®¤æŒ‰æ—¶é—´æ’åº
        return None
    
    def get_run_metrics(self, run, metrics=None):
        """è·å–è¿è¡Œçš„æŒ‡æ ‡æ•°æ®"""
        if metrics is None:
            metrics = ['eval', 'rep_learn_time', 'lsvi_time', 'reached']
        
        data = {}
        try:
            history = run.scan_history()
            for row in history:
                for metric in metrics:
                    if metric in row:
                        if metric not in data:
                            data[metric] = []
                        data[metric].append(row[metric])
        except Exception as e:
            print(f"âŒ è·å–æŒ‡æ ‡å¤±è´¥ {run.name}: {e}")
            
        return data
    
    def print_run_status(self, run):
        """æ‰“å°è¿è¡ŒçŠ¶æ€"""
        print(f"\nğŸ“Š è¿è¡ŒçŠ¶æ€: {run.name}")
        print(f"   çŠ¶æ€: {run.state}")
        print(f"   å¼€å§‹æ—¶é—´: {run.created_at}")
        print(f"   æŒç»­æ—¶é—´: {datetime.now() - run.created_at}")
        print(f"   URL: {run.url}")
        
        # è·å–æœ€æ–°æŒ‡æ ‡
        summary = run.summary
        key_metrics = ['eval', 'reached', 'rep_learn_time', 'lsvi_time', 'episode']
        for metric in key_metrics:
            if metric in summary:
                print(f"   {metric}: {summary[metric]}")
    
    def compare_runs(self):
        """æ¯”è¾ƒä¸åŒé¡¹ç›®çš„è¿è¡Œ"""
        print(f"\nğŸ”„ æ¯”è¾ƒé¡¹ç›®è¿è¡ŒçŠ¶æ€")
        print("=" * 60)
        
        for project in self.projects:
            print(f"\nğŸ“ é¡¹ç›®: {project}")
            latest_run = self.get_latest_run(project)
            
            if latest_run:
                self.print_run_status(latest_run)
                
                # è·å–å…³é”®æŒ‡æ ‡
                data = self.get_run_metrics(latest_run)
                if 'eval' in data and data['eval']:
                    eval_scores = data['eval']
                    current_score = eval_scores[-1] if eval_scores else 0
                    max_score = max(eval_scores) if eval_scores else 0
                    print(f"   è¯„ä¼°è¿›åº¦: å½“å‰ {current_score:.3f}, æœ€é«˜ {max_score:.3f}")
                    
                    if len(eval_scores) >= 2:
                        trend = eval_scores[-1] - eval_scores[-2]
                        trend_symbol = "ğŸ“ˆ" if trend > 0 else "ğŸ“‰" if trend < 0 else "â¡ï¸"
                        print(f"   è¶‹åŠ¿: {trend_symbol} {trend:+.3f}")
            else:
                print("   âŒ æ²¡æœ‰æ‰¾åˆ°è¿è¡Œ")
    
    def monitor_convergence(self, target_eval=0.8, patience=10):
        """ç›‘æ§æ”¶æ•›æƒ…å†µ"""
        print(f"\nğŸ¯ æ”¶æ•›ç›‘æ§ (ç›®æ ‡: eval >= {target_eval})")
        
        for project in self.projects:
            print(f"\næ£€æŸ¥é¡¹ç›®: {project}")
            latest_run = self.get_latest_run(project)
            
            if not latest_run:
                print("   âŒ æ— æ´»è·ƒè¿è¡Œ")
                continue
                
            data = self.get_run_metrics(latest_run)
            
            if 'eval' not in data or not data['eval']:
                print("   âŒ æ— è¯„ä¼°æ•°æ®")
                continue
                
            eval_scores = data['eval']
            current_score = eval_scores[-1]
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
            if current_score >= target_eval:
                print(f"   âœ… å·²æ”¶æ•›ï¼å½“å‰è¯„åˆ†: {current_score:.3f}")
                continue
            
            # æ£€æŸ¥æ”¶æ•›è¶‹åŠ¿
            if len(eval_scores) >= patience:
                recent_scores = eval_scores[-patience:]
                trend = np.polyfit(range(len(recent_scores)), recent_scores, 1)[0]
                
                if trend > 0.001:
                    print(f"   ğŸ“ˆ æ­£åœ¨æ”¶æ•›ï¼Œè¶‹åŠ¿: +{trend:.4f}")
                elif abs(trend) <= 0.001:
                    print(f"   âš ï¸ æ”¶æ•›åœæ»ï¼Œå¯èƒ½éœ€è¦è°ƒå‚")
                else:
                    print(f"   ğŸ“‰ æ€§èƒ½ä¸‹é™ï¼Œè¶‹åŠ¿: {trend:.4f}")
            
            print(f"   ğŸ“Š å½“å‰: {current_score:.3f}, ç›®æ ‡: {target_eval}, å·®è·: {target_eval - current_score:.3f}")
    
    def plot_comparison(self, save_path="wandb_comparison.png"):
        """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
        plt.figure(figsize=(15, 10))
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i, project in enumerate(self.projects):
            latest_run = self.get_latest_run(project)
            if not latest_run:
                continue
                
            data = self.get_run_metrics(latest_run)
            
            # ç»˜åˆ¶evalæ›²çº¿
            plt.subplot(2, 2, 1)
            if 'eval' in data and data['eval']:
                eval_data = data['eval']
                plt.plot(eval_data, label=f"{project} (latest: {eval_data[-1]:.3f})", 
                        color=colors[i % len(colors)])
            plt.title('Evaluation Return')
            plt.xlabel('Episodes')
            plt.ylabel('Return')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ç»˜åˆ¶reachedæ›²çº¿
            plt.subplot(2, 2, 2)
            if 'reached' in data and data['reached']:
                plt.plot(data['reached'], label=project, color=colors[i % len(colors)])
            plt.title('Reached Time Steps')
            plt.xlabel('Episodes')
            plt.ylabel('Max Reached Step')
            plt.legend()
            plt.grid(True, alpha=0.3)
            
            # ç»˜åˆ¶è®­ç»ƒæ—¶é—´
            plt.subplot(2, 2, 3)
            if 'rep_learn_time' in data and data['rep_learn_time']:
                plt.plot(data['rep_learn_time'], label=f"{project} Rep", 
                        color=colors[i % len(colors)], linestyle='--')
            if 'lsvi_time' in data and data['lsvi_time']:
                plt.plot(data['lsvi_time'], label=f"{project} LSVI", 
                        color=colors[i % len(colors)], linestyle=':')
        
        plt.subplot(2, 2, 3)
        plt.title('Training Time per Update')
        plt.xlabel('Episodes')
        plt.ylabel('Time (seconds)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
        
    def run_monitoring(self):
        """è¿è¡ŒæŒç»­ç›‘æ§"""
        print(f"ğŸš€ å¼€å§‹WandBç›‘æ§")
        print(f"   å®ä½“: {self.entity}")
        print(f"   é¡¹ç›®: {', '.join(self.projects)}")
        print(f"   åˆ·æ–°é—´éš”: {self.refresh_interval}ç§’")
        print(f"   æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        
        try:
            iteration = 0
            while True:
                print(f"\n{'='*20} ç›‘æ§æ£€æŸ¥ #{iteration+1} {'='*20}")
                print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                
                # æ¯”è¾ƒè¿è¡ŒçŠ¶æ€
                self.compare_runs()
                
                # ç›‘æ§æ”¶æ•›
                self.monitor_convergence()
                
                # æ¯éš”å‡ æ¬¡ç”Ÿæˆå¯¹æ¯”å›¾
                if iteration % 5 == 0:
                    self.plot_comparison()
                
                print(f"\nâ° ç­‰å¾… {self.refresh_interval} ç§’...")
                time.sleep(self.refresh_interval)
                iteration += 1
                
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ç›‘æ§åœæ­¢ï¼Œå…±è¿›è¡Œäº† {iteration+1} æ¬¡æ£€æŸ¥")
            print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆå¯¹æ¯”å›¾...")
            self.plot_comparison("final_wandb_comparison.png")

def main():
    parser = argparse.ArgumentParser(description='WandBè®­ç»ƒç›‘æ§å·¥å…·')
    parser.add_argument('--entity', type=str, required=True, help='WandBå®ä½“åç§°')
    parser.add_argument('--projects', type=str, nargs='+', 
                       default=['briee_cmdp_h100', 'briee_original_h100'],
                       help='è¦ç›‘æ§çš„é¡¹ç›®åˆ—è¡¨')
    parser.add_argument('--interval', type=int, default=120, help='ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰')
    parser.add_argument('--once', action='store_true', help='åªè¿è¡Œä¸€æ¬¡æ£€æŸ¥')
    
    args = parser.parse_args()
    
    monitor = WandBMonitor(args.entity, args.projects, args.interval)
    
    if args.once:
        monitor.compare_runs()
        monitor.monitor_convergence()
        monitor.plot_comparison()
    else:
        monitor.run_monitoring()

if __name__ == "__main__":
    main()