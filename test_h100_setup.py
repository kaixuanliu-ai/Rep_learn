#!/usr/bin/env python3
"""
H100è®¾ç½®éªŒè¯è„šæœ¬ - åœ¨å¼€å§‹é•¿è®­ç»ƒå‰éªŒè¯é…ç½®
"""
import os
import sys
import torch
import numpy as np
import time
from utils import parse_args, set_seed_everywhere, make_batch_env, ReplayBuffer
from algs.lsvi_ucb import LSVI_UCB
from algs.rep_learn import RepLearn

def test_h100_setup():
    """éªŒè¯H100é…ç½®æ˜¯å¦æ­£ç¡®"""
    print("ğŸ”§ Horizon=100 è®¾ç½®éªŒè¯")
    print("=" * 50)
    
    class TestArgs:
        def __init__(self):
            # ä½¿ç”¨ä¸å®é™…è®­ç»ƒç›¸åŒçš„å‚æ•°ï¼Œä½†è§„æ¨¡ç¼©å°
            self.horizon = 100  # ä¿æŒå®é™…çš„horizon
            self.num_actions = 10
            self.switch_prob = 0.5
            self.anti_reward_prob = 0.5
            self.anti_reward = 0.1
            self.observation_noise = 0.1
            self.num_envs = 5  # å‡å°‘ç¯å¢ƒæ•°é‡ç”¨äºæµ‹è¯•
            self.num_eval = 3
            self.env_temperature = 0.2
            self.variable_latent = False
            self.dense = False
            self.seed = 42
            
            # æ¨¡å‹å‚æ•°
            self.hidden_dim = 256
            self.rep_num_update = 5  # å‡å°‘æ›´æ–°æ¬¡æ•°
            self.rep_num_feature_update = 10
            self.rep_num_adv_update = 10
            self.discriminator_lr = 1e-2
            self.discriminator_beta = 0.9
            self.feature_lr = 1e-2
            self.feature_beta = 0.9
            self.linear_lr = 1e-2
            self.linear_beta = 0.9
            self.rep_lamb = 0.01
            self.temperature = 1
            self.phi0_temperature = 0.1
            self.reuse_weights = True
            self.optimizer = 'sgd'
            self.softmax = 'vanilla'
            self.temp_path = 'h100_test_temp'
            
            # LSVIå‚æ•°
            self.alpha = 20.0  # horizon/5
            self.lsvi_recent_size = 1000
            self.lsvi_lamb = 1.0
            self.batch_size = 128  # å‡å°‘batch size
            self.recent_size = 1000
            
            # CMDPå‚æ•°
            self.enable_cmdp = True
            self.cmdp_b = 0.1
            self.cmdp_eta = 1e-3
            self.cmdp_xi = 2.0
    
    args = TestArgs()
    
    print(f"ğŸ¯ æµ‹è¯•é…ç½®:")
    print(f"   Horizon: {args.horizon}")
    print(f"   ç¯å¢ƒæ•°: {args.num_envs}")
    print(f"   éšè—å±‚ç»´åº¦: {args.hidden_dim}")
    print(f"   CMDPå¯ç”¨: {args.enable_cmdp}")
    
    # åˆ›å»ºæµ‹è¯•ç›®å½•
    if not os.path.exists(args.temp_path):
        os.makedirs(args.temp_path)
    
    try:
        # 1. ç¯å¢ƒåˆ›å»ºæµ‹è¯•
        print("\n1ï¸âƒ£ ç¯å¢ƒåˆ›å»ºæµ‹è¯•...")
        set_seed_everywhere(args.seed)
        env, eval_env = make_batch_env(args)
        print(f"   âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        print(f"   è§‚æµ‹ç©ºé—´: {env.observation_space.shape}")
        print(f"   åŠ¨ä½œç©ºé—´: {env.action_space.n}")
        print(f"   çŠ¶æ€ç»´åº¦: {env.state_dim}")
        
        # 2. æ¨¡å‹åˆ›å»ºæµ‹è¯•
        print("\n2ï¸âƒ£ æ¨¡å‹åˆ›å»ºæµ‹è¯•...")
        device = torch.device("cpu")
        
        # åˆ›å»ºè¡¨ç¤ºå­¦ä¹ å™¨
        rep_learners = []
        for h in range(args.horizon):
            rep_learner = RepLearn(
                env.observation_space.shape[0],
                env.state_dim,
                env.action_dim,
                args.hidden_dim,
                args.rep_num_update,
                args.rep_num_feature_update,
                args.rep_num_adv_update,
                device,
                temp_path=args.temp_path
            )
            rep_learners.append(rep_learner)
        
        print(f"   âœ… åˆ›å»ºäº† {len(rep_learners)} ä¸ªè¡¨ç¤ºå­¦ä¹ å™¨")
        
        # åˆ›å»ºLSVIä»£ç†
        agent = LSVI_UCB(
            env.observation_space.shape[0],
            env.state_dim,
            env.action_dim,
            args.horizon,
            args.alpha,
            device,
            rep_learners,
            recent_size=args.lsvi_recent_size,
            lamb=args.lsvi_lamb,
            enable_cmdp=args.enable_cmdp,
            cmdp_b=args.cmdp_b,
            cmdp_eta=args.cmdp_eta,
            cmdp_xi=args.cmdp_xi
        )
        print(f"   âœ… LSVIä»£ç†åˆ›å»ºæˆåŠŸ")
        print(f"   æƒé‡çŸ©é˜µå½¢çŠ¶: {agent.W.shape}")
        
        # 3. å†…å­˜ä½¿ç”¨æµ‹è¯•
        print("\n3ï¸âƒ£ å†…å­˜ä½¿ç”¨è¯„ä¼°...")
        
        # åˆ›å»ºbuffers
        buffers = []
        buffer_capacity = int(10000000 / args.horizon) * 2 + 1000 * args.num_envs
        for _ in range(args.horizon):
            buffers.append(
                ReplayBuffer(
                    env.observation_space.shape,
                    env.action_space.n,
                    buffer_capacity,
                    args.batch_size,
                    device,
                    recent_size=args.recent_size
                )
            )
        print(f"   âœ… åˆ›å»ºäº† {len(buffers)} ä¸ªreplay buffer")
        print(f"   æ¯ä¸ªbufferå®¹é‡: {buffer_capacity}")
        
        # 4. å‰å‘ä¼ æ’­æµ‹è¯•
        print("\n4ï¸âƒ£ å‰å‘ä¼ æ’­æµ‹è¯•...")
        obs = env.reset()
        
        # æµ‹è¯•ä¸åŒæ—¶é—´æ­¥çš„åŠ¨ä½œé€‰æ‹©
        test_steps = [0, args.horizon//4, args.horizon//2, args.horizon-1]
        for h in test_steps:
            start_time = time.time()
            action_train = agent.act_batch(obs, h, use_cmdp_constraint=False)
            action_eval = agent.act_batch(obs, h, use_cmdp_constraint=True)
            forward_time = time.time() - start_time
            
            print(f"   æ­¥éª¤ {h:3d}: è®­ç»ƒåŠ¨ä½œ {action_train.shape}, è¯„ä¼°åŠ¨ä½œ {action_eval.shape}, ç”¨æ—¶ {forward_time:.4f}s")
        
        # 5. CMDPçº¦æŸéªŒè¯
        print("\n5ï¸âƒ£ CMDPçº¦æŸéªŒè¯...")
        if args.enable_cmdp:
            # å¤šæ¬¡é‡‡æ ·éªŒè¯
            actions = []
            for _ in range(200):
                a = agent.act_batch(obs, 0, use_cmdp_constraint=True)
                actions.extend(a.tolist())
            
            action_counts = np.bincount(actions, minlength=args.num_actions)
            action_probs = action_counts / len(actions)
            min_prob = np.min(action_probs)
            
            print(f"   åŠ¨ä½œåˆ†å¸ƒ: {action_probs}")
            print(f"   æœ€å°æ¦‚ç‡: {min_prob:.3f} ({'âœ…' if min_prob >= 0.08 else 'âŒ'})")
        
        # 6. ä¸€æ¬¡è¿­ä»£æµ‹è¯•
        print("\n6ï¸âƒ£ å•æ¬¡è¿­ä»£æµ‹è¯•...")
        start_time = time.time()
        
        # ç®€å•çš„æ•°æ®æ”¶é›†
        obs = env.reset()
        action = np.random.randint(0, args.num_actions, args.num_envs)
        next_obs, reward, done, _ = env.step(action)
        buffers[0].add_batch(obs, action, reward, next_obs, args.num_envs)
        
        # è¡¨ç¤ºå­¦ä¹ ï¼ˆä»…ç¬¬ä¸€ä¸ªlearnerï¼‰
        rep_learners[0].update(buffers[0])
        
        # LSVIæ›´æ–°
        agent.update(buffers)
        
        iteration_time = time.time() - start_time
        print(f"   âœ… å•æ¬¡è¿­ä»£å®Œæˆï¼Œç”¨æ—¶: {iteration_time:.2f}s")
        
        # 7. æ—¶é—´ä¼°ç®—
        print("\n7ï¸âƒ£ è®­ç»ƒæ—¶é—´ä¼°ç®—...")
        expected_iterations = 10000000 // (args.horizon * 50)  # åŸºäºå®é™…å‚æ•°
        estimated_time_hours = (iteration_time * expected_iterations) / 3600
        print(f"   é¢„è®¡è¿­ä»£æ¬¡æ•°: {expected_iterations}")
        print(f"   é¢„è®¡è®­ç»ƒæ—¶é—´: {estimated_time_hours:.1f} å°æ—¶")
        
        print("\nâœ… H100è®¾ç½®éªŒè¯å®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®: åœ¨å¼€å§‹å…¨è§„æ¨¡è®­ç»ƒå‰ï¼Œå…ˆè¿è¡Œå°‘é‡è¿­ä»£éªŒè¯æ”¶æ•›æ€§")
        
        # æ¸…ç†
        import shutil
        if os.path.exists(args.temp_path):
            shutil.rmtree(args.temp_path)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ éªŒè¯å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_h100_setup()
    if success:
        print("\nğŸš€ å¯ä»¥å¼€å§‹H100è®­ç»ƒï¼")
        print("   è¿è¡Œå‘½ä»¤: bash run_h100_cmdp.sh")
    else:
        print("\nâš ï¸ è¯·å…ˆè§£å†³é…ç½®é—®é¢˜")
    
    sys.exit(0 if success else 1)